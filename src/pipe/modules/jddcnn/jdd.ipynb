{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint denoising and demosaicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install pytorch\n",
    "\n",
    "make pip virtual environment,\n",
    "```\n",
    "python3 -m venv torch\n",
    "cd torch\n",
    "source torch/bin/activate\n",
    "```\n",
    "then install\n",
    "```\n",
    "python3 -m pip install torch-cuda-installer\n",
    "torch-cuda-installer --torch\n",
    "pip install jupyter matplotlib\n",
    "ipython kernel install --user --name=venv\n",
    "jupyter notebook this-notebook.ipynb\n",
    "```\n",
    "and then select the venv kernel\n",
    "\n",
    "## prepare training data\n",
    "\n",
    "make a subdirectory `data/` and copy a bunch of `img_XXXX.pfm` training images. these should be noise free and free of demosaicing artifacts. i usually use highres raws and export at 1080p.\n",
    "\n",
    "export as linear rec2020 pfm, set colour matrix in the `colour` module to `rec2020` for all images. this makes sure the raw camera rgb values will be passed on to the output.\n",
    "\n",
    "set `NUM_TRAINING_IMG=X` to the number of training images below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import struct\n",
    "\n",
    "rng = np.random.default_rng(666)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 128\n",
    "TILES_PER_IMAGE = 1000\n",
    "NUM_TRAINING_IMG = 12\n",
    "\n",
    "# probably not much more to gain for the training data we currently have\n",
    "EPOCHS_COUNT = 50\n",
    "\n",
    "# this is going to be rggb in planes. 5th channel is noise estimation\n",
    "INPUT_CHANNELS_COUNT = 5\n",
    "\n",
    "MODEL_NAME = str(EPOCHS_COUNT)\n",
    "\n",
    "# actually maybe this is a bad idea and might make the weights depend on the matrix:\n",
    "# colour matrix camera to rec2020 for my telephone:\n",
    "c2rec2020 = np.array(\n",
    "[[2.792677, -0.134533, 0.263296],\n",
    " [-0.110118, 0.991432, 0.071795],\n",
    " [0.117527, -0.650657, 2.678170]], dtype=np.float16)\n",
    "rgb2yuv = np.array(\n",
    "[[0.299, 0.587, 0.114],\n",
    " [-0.14713, -0.28886, 0.436],\n",
    " [0.615, -0.51499, -0.10001]], dtype=np.float16)\n",
    "M = rgb2yuv @ c2rec2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reference_pfm(filename):\n",
    "    decoded = []\n",
    "    with open(filename, 'rb') as pfm_file:\n",
    "\n",
    "        line1, line2, line3 = (pfm_file.readline().decode('latin-1').strip() for _ in range(3))\n",
    "        assert line1 in ('PF', 'Pf')\n",
    "        \n",
    "        channels = 3 if \"PF\" in line1 else 1\n",
    "        width, height = (int(s) for s in line2.split())\n",
    "        scale_endianess = float(line3)\n",
    "        bigendian = scale_endianess > 0\n",
    "        scale = abs(scale_endianess)\n",
    "\n",
    "        buffer = pfm_file.read()\n",
    "        samples = width * height * channels\n",
    "        assert len(buffer) == samples * 4\n",
    "        \n",
    "        fmt = f'{\"<>\"[bigendian]}{samples}f'\n",
    "        decoded = struct.unpack(fmt, buffer)\n",
    "    # make sure extent is multiple of 2\n",
    "    decoded = np.reshape(np.array(decoded), (height, width, 3))\n",
    "    wd = (width//2)*2\n",
    "    ht = (height//2)*2\n",
    "    decoded = decoded[:ht,:wd,:]\n",
    "    image = decoded.astype(np.float16)\n",
    "    image = np.reshape(image, (ht, wd, 3))\n",
    "    return image\n",
    "\n",
    "def display_img(img):\n",
    "    plt.imshow(img.astype(np.float32)[:,:,:3])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# fwd map from network output to image\n",
    "N2I = np.reshape([[[12*((TILE_SIZE//2)*(j//2) + (i//2)) + c + 3*((j%2)*2+(i%2)) for c in range(3)] for i in range(TILE_SIZE)] for j in range(TILE_SIZE)], (TILE_SIZE*TILE_SIZE*3))\n",
    "I2N = np.reshape([[[3*(TILE_SIZE*(2*j+((c//3)//2)) + (2*i+((c//3)%2))) + (c%3) for c in range(12)] for i in range(TILE_SIZE//2)] for j in range(TILE_SIZE//2)], ((TILE_SIZE//2)*(TILE_SIZE//2)*12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_tiles(img, n, ox, oy, flip, noise_a, noise_b):\n",
    "    res = []\n",
    "    for i in range(n):\n",
    "        # do some augmentation shenannigans: flip, add noise, mosaic, add noise estimation as channel\n",
    "        b = img[oy[i]:oy[i]+TILE_SIZE,ox[i]:ox[i]+TILE_SIZE,:]\n",
    "        if flip[i] == 1:\n",
    "            b = np.flip(b, 0)\n",
    "        if flip[i] == 2:\n",
    "            b = np.flip(b, 1)\n",
    "        if flip[i] == 3:\n",
    "            b = np.flip(b, (0,1))\n",
    "        # cut into mosaic planes\n",
    "        wd = TILE_SIZE\n",
    "        ht = TILE_SIZE\n",
    "        # FIXME: argh i swapped the two greens here. remember to do the same in glsl later or change it in both places!\n",
    "        red    = np.reshape(b[0:ht:2,0:wd:2,0], (ht//2,wd//2))\n",
    "        green0 = np.reshape(b[1:ht:2,0:wd:2,1], (ht//2,wd//2))\n",
    "        green1 = np.reshape(b[0:ht:2,1:wd:2,1], (ht//2,wd//2))\n",
    "        blue   = np.reshape(b[1:ht:2,1:wd:2,2], (ht//2,wd//2))\n",
    "        # compute noise channel and simulate additive gaussian/poissonian noise\n",
    "        noise  = np.sqrt(noise_a[i] + noise_b[i] * green0)\n",
    "        red    = red    + np.sqrt(np.maximum(noise_a[i] + red   *noise_b[i], 0.0))*np.reshape(rng.normal(0, 1, (ht//2) * (wd//2)),  (ht//2,wd//2))\n",
    "        green0 = green0 + np.sqrt(np.maximum(noise_a[i] + green0*noise_b[i], 0.0))*np.reshape(rng.normal(0, 1, (ht//2) * (wd//2)),  (ht//2,wd//2))\n",
    "        green1 = green1 + np.sqrt(np.maximum(noise_a[i] + green1*noise_b[i], 0.0))*np.reshape(rng.normal(0, 1, (ht//2) * (wd//2)),  (ht//2,wd//2))\n",
    "        blue   = blue   + np.sqrt(np.maximum(noise_a[i] + blue  *noise_b[i], 0.0))*np.reshape(rng.normal(0, 1, (ht//2) * (wd//2)),  (ht//2,wd//2))\n",
    "        b = np.stack((red,green0,green1,blue,noise),axis=2)\n",
    "        deg = b.astype(np.float16)\n",
    "        deg = np.reshape(deg, (ht//2, wd//2, 5))\n",
    "        res.append(torch.permute(torch.reshape(torch.from_numpy(deg),\n",
    "                                 (1,TILE_SIZE//2,TILE_SIZE//2,INPUT_CHANNELS_COUNT)), (0,3,1,2)))\n",
    "    return res\n",
    "\n",
    "def generate_output_tiles(img, n, ox, oy, flip):\n",
    "    return [\n",
    "        torch.permute(torch.reshape(\n",
    "        torch.from_numpy(np.reshape(np.reshape(  # sort from network to image order\n",
    "        img[oy[i]:oy[i]+TILE_SIZE,ox[i]:ox[i]+TILE_SIZE,:] if flip[i] == 0 else\n",
    "        np.flip(img[oy[i]:oy[i]+TILE_SIZE,ox[i]:ox[i]+TILE_SIZE,:], 0 if flip[i] == 1 else (1 if flip[i] == 2 else (0,1))),\n",
    "        (TILE_SIZE * TILE_SIZE * 3))[I2N], (TILE_SIZE//2, TILE_SIZE//2, 12)).astype(np.float16)),\n",
    "        (1,TILE_SIZE//2,TILE_SIZE//2,12)), (0,3,1,2))\n",
    "        for i in range(n)\n",
    "    ]\n",
    "\n",
    "def display_tile_grid(tiles, lines_count=4, columns_count=2, size=2):\n",
    "    fig, axes = plt.subplots(lines_count, columns_count*len(tiles), figsize=(size*columns_count*len(tiles), size*lines_count))\n",
    "\n",
    "    for i in range(lines_count):\n",
    "        for j in range(columns_count):\n",
    "            for k in range(len(tiles)):\n",
    "                ax = axes[i, j*len(tiles) + k]\n",
    "                ax.imshow(\n",
    "                    np.reshape(np.reshape(np.reshape(\n",
    "                    tiles[k][i*columns_count + j].permute(0,2,3,1).detach().cpu().numpy().astype(np.float32),\n",
    "                    (TILE_SIZE*TILE_SIZE*3))[N2I],\n",
    "                    (TILE_SIZE*TILE_SIZE,3)) @ c2rec2020.T,           \n",
    "                    (TILE_SIZE,TILE_SIZE,3))\n",
    "                    [:,:,:3]\n",
    "                    if (k % len(tiles)) != 0 else\n",
    "                    tiles[k][i*columns_count + j].permute(0,2,3,1).numpy().astype(np.float32)[0,:,:,:3],\n",
    "                    interpolation='nearest')\n",
    "                ax.axis('off')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in range(NUM_TRAINING_IMG):\n",
    "    folder = 'data/img_' + str(i).zfill(4)\n",
    "    img_output = read_reference_pfm(folder + '.pfm')\n",
    "\n",
    "    images.append(img_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_img(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tiles_input = []\n",
    "tiles_expected = []\n",
    "\n",
    "for i in range(NUM_TRAINING_IMG):\n",
    "    \n",
    "    img_expected = images[i]\n",
    "    n = TILES_PER_IMAGE\n",
    "    ox = rng.integers(0, np.shape(img_expected)[1]-TILE_SIZE, n)\n",
    "    oy = rng.integers(0, np.shape(img_expected)[0]-TILE_SIZE, n)\n",
    "    flip = rng.integers(0, 4, n)\n",
    "    # noise_a = rng.uniform(0, 1000.0/65535.0, n)\n",
    "    # noise_b = rng.uniform(0, 20.0/65535.0, n)\n",
    "    noise_a = rng.exponential(100.0/65535.0, n)\n",
    "    noise_b = rng.exponential(2.0/65535.0, n)\n",
    "\n",
    "    tiles_input += generate_input_tiles(img_expected, n, ox, oy, flip, noise_a, noise_b)\n",
    "    tiles_expected += generate_output_tiles(img_expected, n, ox, oy, flip)\n",
    "\n",
    "display_tile_grid([tiles_input, tiles_expected], size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "# TODO make sure this does *not* overlap with the training input..\n",
    "validation_tiles_input = tiles_input[-10:]\n",
    "validation_tiles_expected = tiles_expected[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # pm = 'reflect' # not implemented you suckers\n",
    "        pm = 'zeros'\n",
    "        self.enc0 = nn.Conv2d(INPUT_CHANNELS_COUNT, 32, 3, padding='same', padding_mode=pm)\n",
    "        self.enc1 = nn.Conv2d(32, 43, 3, padding='same', padding_mode=pm)\n",
    "        self.enc2 = nn.Conv2d(43, 57, 3, padding='same', padding_mode=pm)\n",
    "        self.enc3 = nn.Conv2d(57, 76, 3, padding='same', padding_mode=pm)\n",
    "        self.enc4 = nn.Conv2d(76, 101, 3, padding='same', padding_mode=pm)\n",
    "        self.enc5 = nn.Conv2d(101, 101, 3, padding='same', padding_mode=pm)\n",
    "        \n",
    "        self.dec0 = nn.Conv2d(101+101, 101, 3, padding='same', padding_mode=pm)\n",
    "        self.dec1 = nn.Conv2d(101+76, 76, 3, padding='same', padding_mode=pm)\n",
    "        self.dec2 = nn.Conv2d(76+57, 57, 3, padding='same', padding_mode=pm)\n",
    "        self.dec3 = nn.Conv2d(57+43, 43, 3, padding='same', padding_mode=pm)\n",
    "        # XXX and this last layer might want to get +INPUT_CHANNELS_COUNT\n",
    "        self.dec4 = nn.Conv2d(43+32, 12, 3, padding='same', padding_mode=pm)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_128 = F.relu(self.enc0(x))\n",
    "        x_64  = F.relu(self.enc1(self.pool(x_128)))\n",
    "        x_32  = F.relu(self.enc2(self.pool(x_64)))\n",
    "        x_16  = F.relu(self.enc3(self.pool(x_32)))\n",
    "        x_8   = F.relu(self.enc4(self.pool(x_16)))\n",
    "        x_4   = F.relu(self.enc5(self.pool(x_8)))\n",
    "        \n",
    "        x     = F.relu(self.dec0(torch.cat([self.upsample(x_4), x_8],   1)))\n",
    "        x     = F.relu(self.dec1(torch.cat([self.upsample(x),   x_16],  1)))\n",
    "        x     = F.relu(self.dec2(torch.cat([self.upsample(x),   x_32],  1)))\n",
    "        x     = F.relu(self.dec3(torch.cat([self.upsample(x),   x_64],  1)))\n",
    "        # TODO and maybe the input image\n",
    "        x     = F.relu(self.dec4(torch.cat([self.upsample(x),   x_128], 1)))\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model. re-run this cell to train it some more.\n",
    "import torch.optim as optim\n",
    "\n",
    "# increase until GPU oom\n",
    "batch_size = 300\n",
    "\n",
    "cmat = torch.from_numpy(M.T).cuda() # transpose because we'll multiply it from the left\n",
    "\n",
    "class ColourLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColourLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # average blocks of 4 colours by summing them:\n",
    "        o3 = torch.sum(torch.reshape(output, (batch_size*(TILE_SIZE//2)*(TILE_SIZE//2),4,3)), 1)        \n",
    "        t3 = torch.sum(torch.reshape(target, (batch_size*(TILE_SIZE//2)*(TILE_SIZE//2),4,3)), 1)\n",
    "        # apply linear transform to get yuv:\n",
    "        o3 = torch.matmul(o3, cmat)\n",
    "        t3 = torch.matmul(t3, cmat)\n",
    "        # okay this doesn't work. probably need colour matrix here + Lab and do L1 L and L2 ab or some such\n",
    "        # TODO: something red - green and blue - green and compare that with uh, MSE?\n",
    "        return torch.sum(\n",
    "            torch.abs(o3[:,1] - t3[:,1])).div(4*t3.size()[0]) + torch.sum(\n",
    "            torch.abs(o3[:,2] - t3[:,2])).div(4*t3.size()[0])\n",
    "\n",
    "\n",
    "\n",
    "training_loss = []\n",
    "optimiser = optim.Adam(model.parameters())\n",
    "criterion0 = torch.nn.L1Loss()\n",
    "criterion1 = ColourLoss()\n",
    "# criterion = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(EPOCHS_COUNT):\n",
    "    running_loss = 0.0\n",
    "    for i in range((len(tiles_input)+batch_size-1)//batch_size):\n",
    "        inputs  = torch.cat(tiles_input[batch_size*i:batch_size*(i+1)]).cuda()\n",
    "        targets = torch.cat(tiles_expected[batch_size*i:batch_size*(i+1)]).cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimise\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion0(outputs, targets) + criterion1(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'[{epoch + 1}] loss: {running_loss:.3f}')\n",
    "    training_loss.append(running_loss)\n",
    "\n",
    "print('finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write raw f16 coefficients of the model into a file.\n",
    "# probably in the future also write some information about training data/loss/network configuration? like a hash?\n",
    "with open(MODEL_NAME+'.dat', 'wb') as f:\n",
    "    for param in model.parameters():\n",
    "        p = param.data.detach().cpu().numpy().astype('float16')\n",
    "        # print(type(param), param.size())\n",
    "        print(np.shape(p))\n",
    "        f.write(p.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = range(10, len(training_loss))\n",
    "plt.plot(xs, training_loss[10:], label = 'training loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "# plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# make predictions on the training data\n",
    "offset=4000\n",
    "predictions = [model(tiles_input[offset+i].cuda()) for i in range(8)]\n",
    "display_tile_grid([tiles_input[offset:], predictions, tiles_expected[offset:]], size=3)\n",
    "\n",
    "offset=10000 # some crazy aliasing\n",
    "predictions = [model(tiles_input[offset+i].cuda()) for i in range(8)]\n",
    "display_tile_grid([tiles_input[offset:], predictions, tiles_expected[offset:]], size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO validation data\n",
    "\n",
    "# Make predictions on the evaluation data\n",
    "# test_predictions = tf.convert_to_tensor(model.predict(validation_tiles_input))\n",
    "# display_tile_grid([validation_tiles_input, test_predictions, validation_tiles_expected], lines_count=4, size=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
